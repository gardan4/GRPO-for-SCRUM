{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a2499d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import anthropic\n",
    "from typing import Dict, List\n",
    "import time\n",
    "from pathlib import Path\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a18faa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Claude client\n",
    "client = anthropic.Anthropic(\n",
    "    api_key=os.environ[\"ANTHROPIC_API_KEY\"]  # Read API key from environment variable\n",
    ")\n",
    "\n",
    "def generate_sprint_context(sprint_data: Dict, style_index: int) -> Dict:\n",
    "    \"\"\"\n",
    "    Generate enhanced context for a single sprint using Claude API\n",
    "    \n",
    "    Args:\n",
    "        sprint_data: Original sprint data with issues\n",
    "        style_index: Which meeting note style to use (0-3)\n",
    "    \n",
    "    Returns:\n",
    "        Enhanced sprint data with context and meeting-style notes\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define meeting note style instructions\n",
    "    style_instructions = [\n",
    "        \"Write the sprint goal as a planning meeting summary with team member mentions and discussion points\",\n",
    "        \"Write the sprint goal as structured sprint planning decisions with priorities and blockers\",\n",
    "        \"Write the sprint goal as conversational team agreement notes\",\n",
    "        \"Write the sprint goal as brief action-oriented notes\"\n",
    "    ]\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "Based on these sprint issues, generate realistic team context and rewrite the sprint goal as meeting notes.\n",
    "\n",
    "Sprint Name: {sprint_data['sprint_name']}\n",
    "Original Sprint Goal: {sprint_data['sprint_goal']}\n",
    "Issues (separated by |||||):\n",
    "{sprint_data['formatted_issues']}\n",
    "\n",
    "Generate a realistic but diverse context including:\n",
    "1. tech_stack: Choose appropriate technologies based on the issues (keep it high-level)\n",
    "2. application_domain: Infer a plausible domain (e.g., \"B2B SaaS platform\", \"E-commerce site\")\n",
    "3. team_composition: Simple team structure (e.g., \"6-person team: 3 backend, 2 frontend, 1 QA\")\n",
    "4. sprint_focus: Main theme based on issues (e.g., \"Security hardening\", \"Feature development\")\n",
    "5. product_stage: General phase (e.g., \"Growing user base\", \"Mature product\")\n",
    "\n",
    "For the sprint goal notes:\n",
    "{style_instructions[style_index]}\n",
    "\n",
    "Make the context realistic and ensure it aligns with the actual issues.\n",
    "\n",
    "Return ONLY a JSON object with this exact structure:\n",
    "{{\n",
    "    \"tech_stack\": \"...\",\n",
    "    \"application_domain\": \"...\",\n",
    "    \"team_composition\": \"...\",\n",
    "    \"sprint_focus\": \"...\",\n",
    "    \"product_stage\": \"...\",\n",
    "    \"sprint_goal_notes\": \"...\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-5-haiku-20241022\",  # Using Haiku for cost efficiency\n",
    "            max_tokens=400,\n",
    "            temperature=0.8,  # Higher temperature for more diversity\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Parse the JSON response\n",
    "        context = json.loads(response.content[0].text)\n",
    "        \n",
    "        # Add the context to the original sprint data\n",
    "        enhanced_sprint = sprint_data.copy()\n",
    "        enhanced_sprint['team_context'] = context\n",
    "        \n",
    "        return enhanced_sprint\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sprint {sprint_data['sprint_name']}: {str(e)}\")\n",
    "        return sprint_data  # Return original if error\n",
    "\n",
    "def process_dataset(input_file: str, output_file: str, start_from: int = 0):\n",
    "    \"\"\"\n",
    "    Process the entire dataset file\n",
    "    \n",
    "    Args:\n",
    "        input_file: Path to input JSONL file\n",
    "        output_file: Path to output JSONL file\n",
    "        start_from: Line number to start from (for resuming)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read all lines first to know the total\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        all_lines = f.readlines()\n",
    "    \n",
    "    total_sprints = len(all_lines)\n",
    "    print(f\"Total sprints to process: {total_sprints}\")\n",
    "    \n",
    "    # Open output file in append mode if resuming\n",
    "    mode = 'a' if start_from > 0 else 'w'\n",
    "    \n",
    "    with open(output_file, mode, encoding='utf-8') as out_f:\n",
    "        for i, line in enumerate(all_lines[start_from:], start=start_from):\n",
    "            try:\n",
    "                # Parse sprint data\n",
    "                sprint_data = json.loads(line.strip())\n",
    "                \n",
    "                # Rotate through meeting note styles\n",
    "                style_index = i % 4\n",
    "                \n",
    "                # Generate enhanced context\n",
    "                enhanced_sprint = generate_sprint_context(sprint_data, style_index)\n",
    "                \n",
    "                # Write to output file\n",
    "                out_f.write(json.dumps(enhanced_sprint) + '\\n')\n",
    "                out_f.flush()  # Ensure data is written\n",
    "                \n",
    "                # Progress update\n",
    "                if (i + 1) % 10 == 0:\n",
    "                    print(f\"Processed {i + 1}/{total_sprints} sprints\")\n",
    "                \n",
    "                # Rate limiting (3 requests per second for Haiku)\n",
    "                time.sleep(0.35)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error on line {i}: {str(e)}\")\n",
    "                # Write original data if enhancement fails\n",
    "                out_f.write(line)\n",
    "                out_f.flush()\n",
    "\n",
    "def estimate_cost(num_sprints: int) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Estimate the cost for processing sprints\n",
    "    \n",
    "    Args:\n",
    "        num_sprints: Number of sprints to process\n",
    "        \n",
    "    Returns:\n",
    "        Cost breakdown\n",
    "    \"\"\"\n",
    "    # Rough estimates\n",
    "    avg_input_tokens = 250  # Sprint data + prompt\n",
    "    avg_output_tokens = 300  # Context + meeting notes\n",
    "    \n",
    "    # Haiku pricing (per million tokens)\n",
    "    input_price = 0.25\n",
    "    output_price = 1.25\n",
    "    \n",
    "    total_input_tokens = num_sprints * avg_input_tokens\n",
    "    total_output_tokens = num_sprints * avg_output_tokens\n",
    "    \n",
    "    input_cost = (total_input_tokens / 1_000_000) * input_price\n",
    "    output_cost = (total_output_tokens / 1_000_000) * output_price\n",
    "    \n",
    "    return {\n",
    "        \"input_cost\": input_cost,\n",
    "        \"output_cost\": output_cost,\n",
    "        \"total_cost\": input_cost + output_cost,\n",
    "        \"per_sprint_cost\": (input_cost + output_cost) / num_sprints\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    INPUT_FILE = \"sprint_goals_training_data-qwen-3B.jsonl\"\n",
    "    OUTPUT_FILE = \"enhanced_sprint_training_data.jsonl\"\n",
    "    \n",
    "    # Estimate cost\n",
    "    cost_estimate = estimate_cost(3000)\n",
    "    print(f\"Estimated costs for 3000 sprints:\")\n",
    "    print(f\"  Input cost: ${cost_estimate['input_cost']:.2f}\")\n",
    "    print(f\"  Output cost: ${cost_estimate['output_cost']:.2f}\")\n",
    "    print(f\"  Total cost: ${cost_estimate['total_cost']:.2f}\")\n",
    "    print(f\"  Per sprint: ${cost_estimate['per_sprint_cost']:.4f}\")\n",
    "    \n",
    "    # Confirm before proceeding\n",
    "    proceed = input(\"\\nProceed with processing? (yes/no): \")\n",
    "    \n",
    "    if proceed.lower() == 'yes':\n",
    "        # Process the dataset\n",
    "        process_dataset(INPUT_FILE, OUTPUT_FILE)\n",
    "        print(\"\\nProcessing complete!\")\n",
    "    else:\n",
    "        print(\"Processing cancelled.\")\n",
    "        \n",
    "    # Example: Resume from line 500 if interrupted\n",
    "    # process_dataset(INPUT_FILE, OUTPUT_FILE, start_from=500)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
